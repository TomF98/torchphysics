{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backup 1: Applying hard constrains\n",
    "\n",
    "For some problems, it is advantageous to apply prior knowledge about the solution in the network architecture, instead of the training process via additional loss terms. For example, in simple domains a Dirichlet boundary condition could be added to the network output with a corresponding characteristic function. Since the boundary condition is then naturally fulfilled, one has to consider fewer terms in the final loss and the optimization may become easier.\n",
    "\n",
    "Here we consider the example: \n",
    "\\begin{align*}\n",
    "    \\partial_y u(x,y) &= \\frac{u(x,y)}{y}, \\text{ in } [0, 1] \\times [0, 1] \\\\\n",
    "    u_1(x, 0) &= 0 , \\text{ for } x \\in [0, 1] \\\\\n",
    "    u_2(x, 1) &= \\sin(20\\pi*x) , \\text{ for } x \\in [0, 1] \\\\\n",
    "    \\vec{n} \\nabla u(x, y) &= 0 , \\text{ for } x \\in \\{0, 1\\}, y \\in \\{0, 1\\}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "This problem has the simple solution $u(x, y) = y\\sin(20\\pi x)$. But because of the high frequencies of the sinus term, a training of the boundary condition is rather problematic. One reason for this is the usage of the MSE inside the loss function, which promotes to just learn the mean value of our boundary function. Another reason is the spectral bias of neural networks.\n",
    "\n",
    "In the following, the problem is firstly implemented the normal way, where all above equations are used to define different loss terms. This will not lead to a correct/useful solution. (Until the first results, no comments will be given regarding the implementation. Since just the basics are used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block is for GPU selection. Please execute.\n",
    "import pathlib\n",
    "import os\n",
    "user = int(str(pathlib.Path().resolve())[22:24])\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= str(user % 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchphysics as tp\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import math\n",
    "\n",
    "# Parameter of the problem\n",
    "width, height = 1.0, 1.0\n",
    "frequenz = 20.0 * math.pi\n",
    "\n",
    "def bc_fn(x):\n",
    "    return torch.cos(frequenz*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we define the space and domain:\n",
    "X = tp.spaces.R1('x')\n",
    "Y = tp.spaces.R1('y')\n",
    "XY = X*Y\n",
    "U = tp.spaces.R1('u')\n",
    "\n",
    "\n",
    "x_axis = tp.domains.Interval(X, 0, 1)\n",
    "y_axis = tp.domains.Interval(Y, 0, 1)\n",
    "square = tp.domains.Parallelogram(XY, [0, 0], [1, 0], [0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create the samplers for the boundary and inside the domain. The boundary samplers, \n",
    "# should sample either for the left and right boundary(x_sampler), or for the top and bottom (y_sampler)\n",
    "inner_sampler = ...\n",
    "y_sampler = ...\n",
    "x_sampler = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tp.models.FCN(input_space=XY, output_space=U, hidden=(50, 50, 50, 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we try to learn the solution without any extensions of the generall PINN approach, e.g. we just use all\n",
    "conditions for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol = 0.0001 # to not divide by small numbers if y is close to 0\n",
    "\n",
    "def pde_residual(u, y):\n",
    "    # TODO implement the PDE\n",
    "    return \n",
    "\n",
    "pde_condition = tp.conditions.PINNCondition(module=model,\n",
    "                                            sampler=inner_sampler,\n",
    "                                            residual_fn=pde_residual,\n",
    "                                            name='pde_condition')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dirichlet_residual(u, x, y):\n",
    "    return u - y * bc_fn(x)\n",
    "\n",
    "# TODO: add the PINNCondition for the Dirichlet boundary\n",
    "dirichlet_condition = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neumann_residual(u, x):\n",
    "    return tp.utils.grad(u, x) # = 0\n",
    "\n",
    "neumann_condition = tp.conditions.PINNCondition(module=model,\n",
    "                                                sampler=x_sampler,\n",
    "                                                residual_fn=neumann_residual,\n",
    "                                                name='neuman_condition', weight=1/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name             | Type       | Params\n",
      "------------------------------------------------\n",
      "0 | train_conditions | ModuleList | 7.9 K \n",
      "1 | val_conditions   | ModuleList | 0     \n",
      "------------------------------------------------\n",
      "7.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "7.9 K     Total params\n",
      "0.031     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "022c52c8dba64f649c72183c081f4f3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomfre/miniconda3/envs/bosch/lib/python3.9/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/tomfre/miniconda3/envs/bosch/lib/python3.9/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d554be39f5fe42e095af39fc57387e0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32b9e76df538477589ad053a87af8bc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start to train all 3 conditions:\n",
    "optim = tp.OptimizerSetting(optimizer_class=torch.optim.Adam, lr=0.001)\n",
    "\n",
    "solver = tp.solver.Solver([pde_condition, dirichlet_condition, neumann_condition], optimizer_setting=optim)\n",
    "\n",
    "trainer = pl.Trainer(gpus=1,\n",
    "                     max_steps=5000,\n",
    "                     logger=False,\n",
    "                     benchmark=True,\n",
    "                     enable_checkpointing=False)\n",
    "                     \n",
    "trainer.fit(solver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training was done with Adam and a learning rate of 0.001 and just for 5000 iterations. But just choosing a different algorithm or lr values will not improve the results easily. The loss will in general hover around some constant value near 0.25.\n",
    "\n",
    "Having a look at the learned solution, most of the time we either get a solution that is close to zero or has just captured a few oscillations of the boundary function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plot_sampler = tp.samplers.PlotSampler(plot_domain=square, n_points=2000)\n",
    "fig = tp.utils.plot(model, lambda u: u, plot_sampler, plot_type='contour_surface')\n",
    "plt.title('learned solution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the error, since we know the analytical solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sampler = tp.samplers.PlotSampler(plot_domain=square, n_points=2000)\n",
    "fig = tp.utils.plot(model, lambda u,x,y: u-torch.cos(20*math.pi*x)*y, plot_sampler, plot_type='contour_surface')\n",
    "plt.title('Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to build the boundary condition into out network.\n",
    "We start by resetting our network (defining a new one). With fewer parameters, because we implement the high frequencies via hard constrains and therefore the network now has to learn a simpler function.\n",
    "\n",
    "For the hard constraints, we create the following python-function, where we choose the ansatz to just multiply the network output with the boundary function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tp.models.FCN(input_space=XY, output_space=U, hidden=(10, 10))\n",
    "\n",
    "def constrain_fn(u, x):\n",
    "    return u*bc_fn(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hard constraints now have to be always be applied inside our residual functions of the different conditions. One important point is, that our constraints do not automatically fulfill any boundary condition. So we still have all previous conditions.\n",
    "\n",
    "Instead, choosing for example the constraint\n",
    "\\begin{equation*}\n",
    "    u*x_1*(1.0 - x_1)*x_2*(1.0 - x_2) + bc_fn(x)\n",
    "\\end{equation*}\n",
    "would naturally fulfill the boundary condition. What we choose is somewhat arbitrary, important for this problem is that the oscillation appears in the constraint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pde_residual(u, x, y):\n",
    "    u = constrain_fn(u, x) # plug in output of model to apply constraint\n",
    "    return tp.utils.grad(u, y) - u/(y + tol)\n",
    "\n",
    "pde_condition = tp.conditions.PINNCondition(module=model,\n",
    "                                            sampler=inner_sampler,\n",
    "                                            residual_fn=pde_residual,\n",
    "                                            name='pde_condition')\n",
    "\n",
    "# TODO: Implement Dirichlet and Neumann residual and condition\n",
    "def dirichlet_residual(u, x, y):\n",
    "    return \n",
    "\n",
    "dirichlet_condition = ...\n",
    "\n",
    "def neumann_residual(u, x):\n",
    "    return \n",
    "\n",
    "neumann_condition = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name             | Type       | Params\n",
      "------------------------------------------------\n",
      "0 | train_conditions | ModuleList | 151   \n",
      "1 | val_conditions   | ModuleList | 0     \n",
      "------------------------------------------------\n",
      "151       Trainable params\n",
      "0         Non-trainable params\n",
      "151       Total params\n",
      "0.001     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7753517e24194525abfb4610cc530c62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomfre/miniconda3/envs/bosch/lib/python3.9/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/tomfre/miniconda3/envs/bosch/lib/python3.9/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa9e3632f6e14203a33d7b398e873ba2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "944d1c463d6a4165823ae9acf8c132b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "optim = tp.OptimizerSetting(optimizer_class=torch.optim.Adam, lr=0.001)\n",
    "\n",
    "solver = tp.solver.Solver([pde_condition, dirichlet_condition, neumann_condition], optimizer_setting=optim)\n",
    "\n",
    "trainer = pl.Trainer(gpus=1,\n",
    "                     max_steps=5000,\n",
    "                     logger=False,\n",
    "                     benchmark=True,\n",
    "                     checkpoint_callback=False)\n",
    "                     \n",
    "trainer.fit(solver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sampler = tp.samplers.PlotSampler(plot_domain=square, n_points=2000)\n",
    "fig = tp.utils.plot(model, constrain_fn, plot_sampler, plot_type='contour_surface')\n",
    "plt.title('learned solution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sampler = tp.samplers.PlotSampler(plot_domain=square, n_points=2000)\n",
    "fig = tp.utils.plot(model, lambda u,x,y: constrain_fn(u,x)-torch.cos(20*math.pi*x)*y, plot_sampler, plot_type='contour_surface')\n",
    "plt.title('Error')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bosch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
