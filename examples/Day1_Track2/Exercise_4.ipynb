{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Physics-informed Operator Learning in TorchPhysics\n",
    "\n",
    "We keep the integral operator given by the ODE\n",
    "\\begin{align*}\n",
    "    \\partial_t u(t) &= 5.0f(t) \\quad \\text{ in } (0, 1), \\\\\n",
    "    u(0) &= 0.\n",
    "\\end{align*}\n",
    "from the previous example. Still, our goal is to train one network that outpus $u$ for a given $f$. However, we now dont have data set consisting of pairs $(u, f)$. Instead we use the PINN-idea from the first examples to train our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block is for GPU selection. Please execute.\n",
    "import os, sys\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "import torch \n",
    "import torchphysics as tp\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "from data_gen_deeponet import integrator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the data as before. The data for $f$ will be used in the training, the data for $u$ only later for validation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"/localdata/tomfre/DeepONet_data_integrator\"\n",
    "input_t = torch.load(f\"{save_path}/input_t.pt\")\n",
    "input_f = torch.load(f\"{save_path}/input_f.pt\")\n",
    "output_u = torch.load(f\"{save_path}/output_u.pt\")\n",
    "\n",
    "#\n",
    "\n",
    "print(\"Time discretization:\", len(input_t))\n",
    "print(\"Available data points:\", len(input_f))\n",
    "print(\"Shape of data:\", input_f.shape)\n",
    "\n",
    "example_idx = 0\n",
    "plt.figure(0, figsize=(6, 3))\n",
    "plt.plot(input_t, input_f[example_idx])\n",
    "plt.plot(input_t, output_u[example_idx])\n",
    "plt.grid()\n",
    "leg = plt.legend([\"f\", \"u\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spaces are the same as in the previous example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = tp.spaces.R1(\"t\")\n",
    "F = tp.spaces.R1(\"f\")\n",
    "U = tp.spaces.R1(\"u\")\n",
    "\n",
    "# TODO: Add the correct function spaces\n",
    "fn_space_F = tp.spaces.FunctionSpace(..., ...) \n",
    "fn_space_U = tp.spaces.FunctionSpace(..., ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that we now use physics-informed loss terms, we also need to define our domain as we did in the first two examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I_t = ... # TODO: define the unit interval (0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still select 80% from our available functions $f$ for the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing:\n",
    "total_data = len(input_f)\n",
    "\n",
    "# TODO: select 80% of total_data and set train_f accordingly.\n",
    "train_N = ...\n",
    "\n",
    "train_f = ...\n",
    "# Note: no train_u here, since we assume this data is not available!\n",
    "\n",
    "test_f = input_f[train_N:]\n",
    "test_u = output_u[train_N:]\n",
    "\n",
    "# Define FunctionSet (DataSet)\n",
    "data_functionset_input = tp.domains.DataFunctionSet(fn_space_F, train_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up are the samplers. Again we need the sampler to load the sample functions $f$. But now we also need to additional samplers that create points in $(0, 1)$ to evaluate our physics informed loss.\n",
    "\n",
    "Note: Our functions $f$ are already discrete and fixed on the grid *input_t*, hence for the *ode_sampler* we create a sampler that just returns the given grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete the FunctionSampler, which should return 6000 samples of f when called.\n",
    "functionsampler_input = tp.samplers.FunctionSamplerRandomUniform(..., ...)\n",
    "\n",
    "ode_sampler = tp.samplers.DataSampler(tp.spaces.Points(input_t, T))\n",
    "initial_sampler = tp.samplers.RandomUniformSampler(I_t.boundary_left, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DeepONet is the same as in the previous example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DeepONet\n",
    "trunk_net = tp.models.FCTrunkNet(T, default_trunk_input=input_t, hidden=(30, 30, 30))\n",
    "branch_net = tp.models.FCBranchNet(fn_space_F, hidden=(50, 50, 50), grid=input_t)\n",
    "\n",
    "model = tp.models.DeepONet(..., ..., ..., output_neurons=50) # TODO: Add the first 3 arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the PINN approach, we now have to create two conditions that implement our problem.\n",
    "Starting with the ODE $\\partial_t u - 5.0 f = 0$.\n",
    "\n",
    "As for the *PINNCondition*, we have the *PIDeepONetCondition* that handles the implementation internally. We only have to finish the residual function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Finish the residual function\n",
    "def ode_residual(...):\n",
    "    return ...\n",
    "\n",
    "ode_cond = tp.conditions.PIDeepONetCondition(deeponet_model=model, \n",
    "                                             branch_function_sampler=functionsampler_input, \n",
    "                                             trunk_points_sampler=ode_sampler, \n",
    "                                             residual_fn=ode_residual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is the initial condition $u(0)=0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Finish the initial residual\n",
    "def initial_residual(u):\n",
    "    return ...\n",
    "\n",
    "# TODO: Add the correct arguments to the condition\n",
    "initial_cond = tp.conditions.PIDeepONetCondition(deeponet_model=..., \n",
    "                                                 branch_function_sampler=..., \n",
    "                                                 trunk_points_sampler=..., \n",
    "                                                 residual_fn=...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we start the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = torch.optim.lr_scheduler.StepLR\n",
    "\n",
    "optim = tp.OptimizerSetting(optimizer_class=torch.optim.Adam, lr=0.002,\n",
    "                            scheduler_class=lr_scheduler, \n",
    "                            scheduler_args={\"step_size\": 2500, \"gamma\":0.2})\n",
    "solver = tp.solver.Solver([ode_cond, initial_cond], optimizer_setting=optim)\n",
    "\n",
    "trainer = pl.Trainer(devices=1, accelerator=\"gpu\",\n",
    "                    num_sanity_val_steps=0,\n",
    "                    benchmark=True,\n",
    "                    max_steps=10000, \n",
    "                    logger=False, \n",
    "                    enable_checkpointing=False)\n",
    "\n",
    "trainer.fit(solver)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check the perfomance on the unseen testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output = model(branch_inputs=tp.spaces.Points(test_f, F)).as_tensor.detach()\n",
    "rel_error = torch.max(torch.abs(model_output - test_u)) / torch.max(torch.abs(test_u))\n",
    "print(f\"Relative error on test data: {rel_error*100} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_idx = 340\n",
    "\n",
    "print(\"Example plot:\")\n",
    "f, axarr = plt.subplots(1,2, figsize=(9, 2.5))\n",
    "axarr[0].plot(input_t, test_f[plot_idx])\n",
    "axarr[0].title.set_text(r\"Example $f$\")\n",
    "axarr[0].grid()\n",
    "axarr[1].plot(input_t, test_u[plot_idx])\n",
    "axarr[1].plot(input_t, model_output[plot_idx], linestyle=\"--\")\n",
    "axarr[1].title.set_text(r\"Solution $u$\")\n",
    "axarr[1].grid()\n",
    "leg = axarr[1].legend([\"True solution\", \"Predicted solution\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now ask ourselfs how well the model performs on data not included in the dataset.\n",
    "The training consits of oscillating functions and polynomials. So let us check how well the model performs on step functions. Choose an $t_0 \\in (0, 1)$ and define \n",
    "\\begin{equation}\n",
    "    f(t)  = \\begin{cases}\n",
    "            1 \\quad &\\text{if } t \\geq t_0, \\\\\n",
    "            0 \\quad &\\text{if } t < t_0.\n",
    "        \\end{cases}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_f = torch.zeros((1, len(input_t), 1))\n",
    "\n",
    "#####################################################\n",
    "# TODO: Here implement the step function using \"input_t\" and \"t_0\":\n",
    "t_0 = ...\n",
    "new_f[0] = ...\n",
    "\n",
    "\n",
    "#####################################################\n",
    "expected_u = integrator(input_t, new_f)\n",
    "model_output = model(branch_inputs=tp.spaces.Points(new_f, F)).as_tensor.detach()\n",
    "print(\"Test:\")\n",
    "f, axarr = plt.subplots(1,2, figsize=(9, 2.5))\n",
    "axarr[0].plot(input_t, new_f[0])\n",
    "axarr[0].title.set_text(r\"Example $f$\")\n",
    "axarr[0].grid()\n",
    "axarr[1].plot(input_t, expected_u[0])\n",
    "axarr[1].plot(input_t, model_output[0], linestyle=\"--\")\n",
    "axarr[1].title.set_text(r\"Solution $u$\")\n",
    "axarr[1].grid()\n",
    "leg = axarr[1].legend([\"True solution\", \"Predicted solution\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prefomance should be not so good. But we can maybe add some example functions to our train set and hope that this improves the result!\n",
    "The following code needs to be copied in the second cell, after we load the data, to add some steps functions to our training set. Then run the notebook again and see if this improves the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_f = torch.zeros((500, len(input_t), 1))\n",
    "t_0_random = torch.rand((500, 1, 1))\n",
    "step_f[:] = (t_0_random[:] <= input_t)\n",
    "step_u = integrator(input_t, step_f)\n",
    "\n",
    "input_f = torch.cat((step_f, input_f), dim=0)\n",
    "output_u = torch.cat((step_u, output_u), dim=0)\n",
    "\n",
    "# TODO: Add the above code to the second cell after the loading of the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tp_version2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
