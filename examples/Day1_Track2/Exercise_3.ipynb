{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Operator Learning in TorchPhysics\n",
    "\n",
    "This notebook contains a template for the joint exercise regarding operator learning utilities of TorchPhysics.\n",
    "\n",
    "We start easy and try to learn the integral operator given by the ODE\n",
    "\\begin{align*}\n",
    "    \\partial_t u(t) &= 5.0f(t) \\quad \\text{ in } (0, 1), \\\\\n",
    "    u(0) &= 0.\n",
    "\\end{align*}\n",
    "Goal is to train one network that outpus $u$ for a given $f$. \n",
    "\n",
    "We will use the Deep Operator Networks (DeepONet) [(paper)](https://arxiv.org/abs/2103.10974) to solve this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block is for GPU selection. Please execute.\n",
    "import os, sys\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "import torch \n",
    "import torchphysics as tp\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that DeepONets are a data-driven approach we first have to load some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time discretization: 100\n",
      "Available data points: 20000\n",
      "Shape of data: torch.Size([20000, 100, 1])\n"
     ]
    }
   ],
   "source": [
    "save_path = \"/localdata/tomfre/DeepONet_data_integrator\"\n",
    "input_t = torch.load(f\"{save_path}/input_t.pt\")\n",
    "input_f = torch.load(f\"{save_path}/input_f.pt\")\n",
    "output_u = torch.load(f\"{save_path}/output_u.pt\")\n",
    "\n",
    "print(\"Time discretization:\", len(input_t))\n",
    "print(\"Available data points:\", len(input_f))\n",
    "print(\"Shape of data:\", input_f.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can have a look at one example of our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_idx = 102\n",
    "plt.figure(0, figsize=(6, 3))\n",
    "plt.plot(input_t, input_f[example_idx])\n",
    "plt.plot(input_t, output_u[example_idx])\n",
    "plt.grid()\n",
    "leg = plt.legend([\"f\", \"u\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we implement the *Spaces* that appear in the problem. Now we have additional *FunctionSpaces* defining what kind of functions appear in the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = ...\n",
    "F = ...\n",
    "U = ...\n",
    "\n",
    "fn_space_F = ...\n",
    "fn_space_U = tp.spaces.FunctionSpace(T, U)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split our data into training and testing set. Such that we only train on one part of the data and afterwards can validate our model on the unseen testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing:\n",
    "total_data = len(input_f)\n",
    "train_N = int(0.8*total_data) # we use 80% of the data for training\n",
    "\n",
    "train_f = ...\n",
    "train_u = ...\n",
    "\n",
    "test_f = ...\n",
    "test_u = ...\n",
    "\n",
    "# Define FunctionSet (similar to a PyTorch DataSet) to handle data collection.\n",
    "data_functionset_input = tp.domains.DataFunctionSet(...)\n",
    "data_functionset_output = tp.domains.DataFunctionSet(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We marked what data belongs to what space, now we also create an object that selects some samples from our data and passes it to our model for training.\n",
    "This is done by the *FunctionSampler*-class. They are comparable to the *Samplers* from before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functionsampler_input = tp.samplers.FunctionSamplerRandomUniform(...)\n",
    "functionsampler_output = tp.samplers.FunctionSamplerCoupled(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define our neural network that should learn the operator. As explained the DeepONet consists of two different parts, the *TrunkNet* and the *BranchNet*. They are created separately, one could just for them abritrary architectures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DeepONet\n",
    "trunk_net = tp.models.FCTrunkNet(..., hidden=(30, 30, 30))\n",
    "branch_net = tp.models.FCBranchNet(..., hidden=(50, 50, 50))\n",
    "model = tp.models.DeepONet(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the training we again need to define a condition that defines what model should learn what kind of equation. Since we are only working with a data-driven learning objective, this is quite simple here.\n",
    "The *OperatorCondition* will plug the data $X_i$ provided from one sampler into the model and compares the output $u_\\theta(X_i)$ with the data $Y_i$ of the second sampler by computing the mean squarred error:\n",
    "\\begin{align*}\n",
    "    \\frac{1}{N}\\sum_{i=1}^N \\|u_\\theta(X_i) - Y_i\\|^2\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_condition = tp.conditions.OperatorCondition(module=..., \n",
    "                                                 input_function_sampler=..., \n",
    "                                                 output_function_sampler=...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the training. Where we now also add a *learning rate scheduler* to decrease the learning rate after a given number of steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = torch.optim.lr_scheduler.StepLR\n",
    "\n",
    "optim = tp.OptimizerSetting(optimizer_class=torch.optim.Adam, lr=0.005,\n",
    "                            scheduler_class=lr_scheduler, \n",
    "                            scheduler_args={\"step_size\": 3000, \"gamma\":0.25})\n",
    "\n",
    "# As before define the solver and trainer:\n",
    "solver = tp.solver.Solver([data_condition], optimizer_setting=optim)\n",
    "\n",
    "trainer = pl.Trainer(devices=1, accelerator=\"gpu\",\n",
    "                    num_sanity_val_steps=0,\n",
    "                    benchmark=True,\n",
    "                    max_steps=20000, \n",
    "                    logger=False, \n",
    "                    enable_checkpointing=False)\n",
    "\n",
    "trainer.fit(solver)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we check the performance on the test data (which the model has not seen before!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative error on test data: 5.343601703643799 %\n"
     ]
    }
   ],
   "source": [
    "model_output = model(branch_inputs=tp.spaces.Points(test_f, F)).as_tensor.detach()\n",
    "rel_error = torch.max(torch.abs(model_output - test_u)) / torch.max(torch.abs(test_u))\n",
    "print(f\"Relative error on test data: {rel_error*100} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize the performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Example plot:\")\n",
    "plot_idx = 620\n",
    "f, axarr = plt.subplots(1,2, figsize=(9, 2.5))\n",
    "axarr[0].plot(input_t, test_f[plot_idx])\n",
    "axarr[0].title.set_text(r\"Example $f$\")\n",
    "axarr[0].grid()\n",
    "axarr[1].plot(input_t, test_u[plot_idx])\n",
    "axarr[1].plot(input_t, model_output[plot_idx], linestyle=\"--\")\n",
    "axarr[1].title.set_text(r\"Solution $u$\")\n",
    "axarr[1].grid()\n",
    "leg = axarr[1].legend([\"True solution\", \"Predicted solution\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tp_version2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
