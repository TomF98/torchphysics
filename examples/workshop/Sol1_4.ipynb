{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise Sheet 1\n",
    "\n",
    "#### 1.4 Learning an Inverse Operator \n",
    "We consider the ODE: \n",
    "\\begin{align*}\n",
    "     \\partial_t^2 u(t) &= -ku(t), \\text{ on } [0, 2\\pi]\\\\\n",
    "     u(0) = &1.0, \\, \\partial_t u(0) = 0\n",
    "\\end{align*}\n",
    "The solution is given by $u(t; k) = u_0 \\cdot \\sin(\\sqrt{k} t)$. \n",
    "\n",
    "We want to train a neural network, that given solution data $(u(t_0), \\dots, u(t_N))$ should return the corresponding parameter $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import math\n",
    "# Here all parameters are defined:\n",
    "t_min, t_max = 0.0, 2*math.pi\n",
    "k_min, k_max = 0.1, 4.0\n",
    "\n",
    "\n",
    "# dataset size for training and testing \n",
    "N_t = 100 # discrete number of time points (t_0, ..., t_N)\n",
    "N_k_train = 500\n",
    "N_k_test = 20\n",
    "\n",
    "train_iterations = 5000\n",
    "learning_rate = 1.e-3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a) Creating the Dataset\n",
    "For the training of the neural network, we first need to create a fitting dataset. Create four tensors `input_training, output_training, input_testing, output_testing`. With shapes and data:\n",
    "```\n",
    "input_training.shape = [N_k_train, N_t], output_training.shape = [N_k_train, 1] \n",
    "```\n",
    "```\n",
    "input_training[i] = (u(t_0; k_i), ..., u(t_N; k_i)), output_training[i] = k_i\n",
    "```\n",
    "Similar for the testing case. Here we want to sample $k_i$, for $i=1,\\dots,N_{k_{train}}$, randomly in our given interval (`torch.rand`) and use an equidistant grid for $t$ (`torch.linspace`). For the implementation of $u$, the functions `torch.sin` and `torch.sqrt` are helpful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Create and fill the tensors\n",
    "# input_training  = \n",
    "# output_training = \n",
    "# input_testing   = \n",
    "# output_testing  = \n",
    "\n",
    "def u(t, k):\n",
    "    return torch.sin(torch.sqrt(k)*t)\n",
    "\n",
    "\n",
    "input_training = torch.zeros((N_k_train, N_t))\n",
    "output_training = torch.zeros((N_k_train, 1))\n",
    "\n",
    "input_testing = torch.zeros((N_k_test, N_t))\n",
    "output_testing = torch.zeros((N_k_test, 1)) \n",
    "\n",
    "t_grid = torch.linspace(t_min, t_max, N_t)\n",
    "D_values = k_min + (k_max - k_min) * torch.rand(N_k_train).reshape(-1, 1)\n",
    "D_values_test = k_min + (k_max - k_min) * torch.rand(N_k_test).reshape(-1, 1)\n",
    "\n",
    "input_training = u(t_grid, D_values)\n",
    "input_testing  = u(t_grid, D_values_test)\n",
    "output_training = D_values\n",
    "output_testing  = D_values_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b) Defining the Neural Network\n",
    "Build a network that has $N_t$ input neurons and 1 output neuron for $k$, two hidden layers of size 25 and `torch.nn.Tanh` as activations in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: implement the neural network\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(N_t, 25), torch.nn.Tanh(), \n",
    "    torch.nn.Linear(25, 25), torch.nn.Tanh(), \n",
    "    torch.nn.Linear(25, 1)\n",
    ") "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### c) Writing the Training Loop\n",
    "Complete the trainig loop, like in the exercise 1.2 and 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at iteration 0 / 5000 is 5.668654\n",
      "Loss at iteration 249 / 5000 is 0.006786\n",
      "Loss at iteration 499 / 5000 is 0.000377\n",
      "Loss at iteration 749 / 5000 is 0.000109\n",
      "Loss at iteration 999 / 5000 is 0.000048\n",
      "Loss at iteration 1249 / 5000 is 0.000024\n",
      "Loss at iteration 1499 / 5000 is 0.000013\n",
      "Loss at iteration 1749 / 5000 is 0.000008\n",
      "Loss at iteration 1999 / 5000 is 0.000005\n",
      "Loss at iteration 2249 / 5000 is 0.000003\n",
      "Loss at iteration 2499 / 5000 is 0.000002\n",
      "Loss at iteration 2749 / 5000 is 0.000002\n",
      "Loss at iteration 2999 / 5000 is 0.000001\n",
      "Loss at iteration 3249 / 5000 is 0.000001\n",
      "Loss at iteration 3499 / 5000 is 0.000001\n",
      "Loss at iteration 3749 / 5000 is 0.000001\n",
      "Loss at iteration 3999 / 5000 is 0.000001\n",
      "Loss at iteration 4249 / 5000 is 0.000005\n",
      "Loss at iteration 4499 / 5000 is 0.000034\n",
      "Loss at iteration 4749 / 5000 is 0.000001\n",
      "Loss at iteration 4999 / 5000 is 0.000001\n"
     ]
    }
   ],
   "source": [
    "### Move data to GPU\n",
    "model.to(\"cuda\")\n",
    "input_training = input_training.to(\"cuda\")\n",
    "output_training = output_training.to(\"cuda\")\n",
    "\n",
    "### For the loss, we take the mean squared error and Adam for optimization.\n",
    "loss_fn = torch.nn.MSELoss() \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "### Training loop\n",
    "for t in range(train_iterations):\n",
    "    ### TODO: Model evaluation, loss computation and optimization\n",
    "    model_out = model(input_training)\n",
    "\n",
    "    loss = loss_fn(model_out, output_training)\n",
    "\n",
    "    ### Shows current loss every 250 iterations:\n",
    "    if t == 0 or (t+1) % 250 == 0:\n",
    "        print(\"Loss at iteration %i / %i is %f\" %(t, train_iterations, loss.item()))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at iteration 0 / 5000 is 5.668654\n",
      "Loss at iteration 249 / 5000 is 0.006786\n",
      "Loss at iteration 499 / 5000 is 0.000377\n",
      "Loss at iteration 749 / 5000 is 0.000109\n",
      "Loss at iteration 999 / 5000 is 0.000048\n",
      "Loss at iteration 1249 / 5000 is 0.000024\n",
      "Loss at iteration 1499 / 5000 is 0.000013\n",
      "Loss at iteration 1749 / 5000 is 0.000008\n",
      "Loss at iteration 1999 / 5000 is 0.000005\n",
      "Loss at iteration 2249 / 5000 is 0.000003\n",
      "Loss at iteration 2499 / 5000 is 0.000002\n",
      "Loss at iteration 2749 / 5000 is 0.000002\n",
      "Loss at iteration 2999 / 5000 is 0.000001\n",
      "Loss at iteration 3249 / 5000 is 0.000001\n",
      "Loss at iteration 3499 / 5000 is 0.000001\n",
      "Loss at iteration 3749 / 5000 is 0.000001\n",
      "Loss at iteration 3999 / 5000 is 0.000001\n",
      "Loss at iteration 4249 / 5000 is 0.000005\n",
      "Loss at iteration 4499 / 5000 is 0.000034\n",
      "Loss at iteration 4749 / 5000 is 0.000001\n",
      "Loss at iteration 4999 / 5000 is 0.000001\n"
     ]
    }
   ],
   "source": [
    "### Move data to GPU\n",
    "model.to(\"cuda\")\n",
    "input_training = input_training.to(\"cuda\")\n",
    "output_training = output_training.to(\"cuda\")\n",
    "\n",
    "### For the loss, we take the mean squared error and Adam for optimization.\n",
    "loss_fn = torch.nn.MSELoss() \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "### Training loop\n",
    "for t in range(train_iterations):\n",
    "    ### TODO: Model evaluation, loss computation and optimization\n",
    "    model_out = model(input_training)\n",
    "\n",
    "    loss = loss_fn(model_out, output_training)\n",
    "\n",
    "    ### Shows current loss every 250 iterations:\n",
    "    if t == 0 or (t+1) % 250 == 0:\n",
    "        print(\"Loss at iteration %i / %i is %f\" %(t, train_iterations, loss.item()))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative error on the test data is: tensor(0.0006, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "### Compute error:\n",
    "model.to(\"cpu\")\n",
    "model_out = model(input_testing)\n",
    "error = torch.abs(model_out - output_testing)\n",
    "print(\"Relative error on the test data is:\", torch.max(error) / torch.max(output_testing))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bosch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
